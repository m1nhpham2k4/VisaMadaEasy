import os
import re
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
import unicodedata

def clean_text(text):
    text = unicodedata.normalize("NFC", text)
    text = text.replace('\xa0', ' ').replace('\r', ' ')
    text = re.sub(r'\s+', ' ', text)
    lines = [line.strip() for line in text.splitlines() if line.strip()]
    return ' '.join(lines)

def chunk_qa_steps(text, max_chunk_length=500):
    # Chia theo cÃ¡c "BÆ°á»›c X -"
    parts = re.split(r'(BÆ°á»›c \d+ - )', text)
    chunks = []
    
    # Náº¿u cÃ³ Ä‘á»‹nh dáº¡ng theo "BÆ°á»›c", giá»¯ nguyÃªn má»—i bÆ°á»›c lÃ m 1 Ä‘oáº¡n
    if len(parts) > 1:
        for i in range(1, len(parts), 2):
            title = parts[i].strip()
            content = parts[i + 1].strip()
            full = f"{title}{content}"

            # Chia Ä‘oáº¡n dÃ i thÃ nh nhiá»u Ä‘oáº¡n nhá» hÆ¡n
            if len(full) > max_chunk_length:
                sub_chunks = re.findall(r'.{1,' + str(max_chunk_length) + r'}(?:(?<=\n)|(?=\s)|$)', full)
                chunks.extend([sc.strip() for sc in sub_chunks if sc.strip()])
            else:
                chunks.append(full.strip())
    else:
        # fallback náº¿u khÃ´ng theo "BÆ°á»›c", chia Ä‘á»u má»—i N kÃ½ tá»±
        raw_chunks = re.findall(r'.{1,' + str(max_chunk_length) + r'}(?:(?<=\n)|(?=\s)|$)', text)
        chunks = [c.strip() for c in raw_chunks if c.strip()]

    return chunks


def build_faiss_from_folder(folder_path: str, index_path: str = "faiss.index", chunks_path: str = "faiss_chunks.npy"):
    model = SentenceTransformer('all-MiniLM-L6-v2')
    all_chunks = []
    txt_file_count = 0

    print(f"ðŸ” Äang duyá»‡t thÆ° má»¥c: {folder_path}")
    for filename in os.listdir(folder_path):
        if filename.endswith(".txt"):
            txt_file_count += 1
            file_path = os.path.join(folder_path, filename)
            print(f"ðŸ“„ Äang xá»­ lÃ½: {filename}")
            try:
                with open(file_path, "r", encoding="utf-8-sig") as f:
                    content = f.read()
            except UnicodeDecodeError:
                print(f"âš ï¸ KhÃ´ng Ä‘á»c Ä‘Æ°á»£c file (encoding lá»—i): {filename}")
                continue

            cleaned = clean_text(content)
            chunks = chunk_qa_steps(cleaned)
            all_chunks.extend(chunks)

            # Debug: in cÃ¡c Ä‘oáº¡n cÃ³ chá»©a "phÃ¡p"
            for chunk in chunks:
                if "phÃ¡p" in chunk.lower():
                    print("ðŸ“Œ Äoáº¡n chá»©a 'PhÃ¡p':")
                    print(chunk[:300])
                    print("------")

    if not all_chunks:
        print("âŒ KhÃ´ng tÃ¬m tháº¥y Ä‘oáº¡n nÃ o Ä‘á»ƒ táº¡o FAISS.")
        return

    print(f"ðŸ“ Tá»•ng sá»‘ file .txt Ä‘Ã£ xá»­ lÃ½: {txt_file_count}")
    print(f"ðŸ”¹ Tá»•ng sá»‘ Ä‘oáº¡n chunk: {len(all_chunks)}")
    print("ðŸ”¹ Äang táº¡o embedding...")
    embeddings = model.encode(all_chunks, convert_to_numpy=True)

    print("ðŸ”¹ Äang táº¡o FAISS index...")
    index = faiss.IndexFlatL2(embeddings.shape[1])
    index.add(embeddings)

    print("ðŸ’¾ Äang lÆ°u FAISS index vÃ  chunk text...")
    faiss.write_index(index, index_path)
    np.save(chunks_path, np.array(all_chunks))

    print(f"âœ… Xong! ÄÃ£ lÆ°u FAISS táº¡i '{index_path}', chunks táº¡i '{chunks_path}'")

# Gá»i hÃ m
if __name__ == "__main__":
    build_faiss_from_folder("./data")
